{"cells":[{"cell_type":"markdown","metadata":{"id":"3uelSgx7FpzK"},"source":["## **Basics of PyTorch**\n","PyTorch is an open-source machine learning library widely used for applications such as computer vision, natural language processing, and more. It is favored by researchers and practitioners due to its dynamic computation graph, which allows for more flexibility and easier debugging.\n","At the core of PyTorch are tensors.\n","\n","In this notebook, we will cover the basics of PyTorch and tensors, including:\n","\n","1. Installation: How to install PyTorch on your machine.\n","\n","2. Creating Tensors: Various ways to create tensors in PyTorch.\n","\n","3. Tensor Operations: Basic operations that can be performed on tensors.\n","\n","By the end of this notebook, you should have a solid understanding of how to use PyTorch and tensors for basic machine learning tasks. Let's get started!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vyQdUa2UFpzS"},"source":["#### **Prerequisites**\n","1. Make sure Python version is `3.8` or greater.\n","\n","2. It is recommended, but not required, that your system has an NVIDIA GPU in order to harness\n","the full power of PyTorch’s CUDA support. To use CUDA follow Nvidia's Installation Guide for Windows and Linux.\n","\n"," **All tasks performed today can be done using CPU alone. PyTorch defaults to storing tensors on cpu.**\n"]},{"cell_type":"markdown","metadata":{"id":"7jxRf5orFpzY"},"source":["#### **Installation**\n","To get started with PyTorch, you need to have it installed on your system.\n","\n","1. **Windows**\n","- To install without CUDA\n","`pip3 install torch torchvision torchaudio`\n","\n","- To install with CUDA\n","`pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121`\n","\n","2. **Mac**\n","- CUDA is not available on MacOS, therefore default package is installed\n","`pip3 install torch torchvision torchaudio`\n","\n","3. **Linux**\n","- To install without CUDA\n","`pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu`\n","\n","- To install with CUDA\n","`pip3 install torch torchvision torchaudio`\n","\n","\n","For further details, you can also use the [PyTorch start locally guide](https://pytorch.org/get-started/locally/) on PyTorch's website."]},{"cell_type":"markdown","metadata":{"id":"MwtbyxS0Fpzd"},"source":["### **Tensors**\n","\n","At the core of PyTorch are `tensors`. Tensors are multi-dimensional arrays that are similar to NumPy arrays but with additional capabilities for GPU acceleration. Understanding tensors is fundamental to using PyTorch effectively, as they are the primary data structure used for storing and manipulating data in deep learning models."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"38yMtXh3Fpzf"},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","metadata":{"id":"URhdqumiFpzi"},"source":["Tensor Creation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJrdnXEkFpzk","outputId":"1ff83054-fdc8-4a05-d962-c58ecb61554f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2],\n","        [3, 4]])\n"]}],"source":["#Directly from data\n","data = [[1, 2], [3, 4]]\n","x_data = torch.tensor(data)\n","print(x_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmxEmPMlFpzo","outputId":"041ca1c8-2d7d-4caf-aa1e-2346144b04ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2],\n","        [3, 4]])\n"]}],"source":["# From Numpy array\n","import numpy as np\n","np_array = np.array(data)\n","x_np = torch.from_numpy(np_array)\n","print(x_np)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akODrBu8Fpzq","outputId":"5a412de5-a75c-4a8c-a2be-b713468f811c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ones Tensor: \n"," tensor([[1, 1],\n","        [1, 1]]) \n","\n","Random Tensor: \n"," tensor([[0.5642, 0.6910],\n","        [0.1003, 0.9418]]) \n","\n"]}],"source":["# From other tensors\n","x_ones = torch.ones_like(x_data) # retains the properties of x_data\n","print(f\"Ones Tensor: \\n {x_ones} \\n\")\n","x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n","print(f\"Random Tensor: \\n {x_rand} \\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vR_ZHRvcFpzv","outputId":"9cdf1307-77a2-4936-ebce-b8ba9277fea3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Tensor: \n"," tensor([[0.9631, 0.4014, 0.7564],\n","        [0.5782, 0.5438, 0.1929]]) \n","\n","Ones Tensor: \n"," tensor([[1., 1., 1.],\n","        [1., 1., 1.]]) \n","\n","Zeros Tensor: \n"," tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n","Empty Tensor: \n"," tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n"]}],"source":["# Populating tensors with random/constant values\n","\n","shape = (2, 3,) # tuple of tensor dimensions\n","ones_tensor = torch.ones(shape)\n","zeros_tensor = torch.zeros(shape)\n","torch.manual_seed(1789)\n","rand_tensor = torch.rand(shape)\n","empty_tensor = torch.empty(shape) # memory is allocated, tensor populated with garbage- values\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n","print(f\"Empty Tensor: \\n {empty_tensor}\")\n"]},{"cell_type":"markdown","metadata":{"id":"B3la7sKdFpz0"},"source":["Using `seed` allows you to reproduce same random tensors across runs. When you set a seed for a random number generator, you initialize it to a specific value, which makes sure that the sequence of random numbers generated is the same every time you run your code. This is especially useful in research settings - where you’ll want some assurance of the reproducibility of your results."]},{"cell_type":"markdown","metadata":{"id":"lgMI2L1zFpz3"},"source":["#### **Tensor Attributes**\n","Tensor attributes describe their shape, datatype, and the device on which they are stored.\n"]},{"cell_type":"markdown","metadata":{"id":"ED-woQ9eFpz5"},"source":["1. **Shape** - The extent of each dimension of a tensor\n","   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sh75PbBMFpz6","outputId":"0b5dff98-38d5-4ddf-af1d-f3c0b05b7837"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 2, 3]) \n","\n","tensor([[[0., 0., 0.],\n","         [0., 0., 0.]],\n","\n","        [[0., 0., 0.],\n","         [0., 0., 0.]]])\n"]}],"source":["x = torch.empty(2, 2, 3)\n","print(x.shape,\"\\n\")\n","print(x)\n"]},{"cell_type":"markdown","metadata":{"id":"SXV-sh2SFpz7"},"source":["2. **Datatype** - An object that represents the data type of a `torch.Tensor`. PyTorch has twelve different data types. They offer various precision and range options fo numerical operations in PyTorch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_GMjKtGZFpz8","outputId":"43e5f629-ce7e-47e0-cb30-d741e3baff16"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.int16\n","torch.float64\n","torch.int8\n","torch.int32\n","torch.int64\n","torch.bool\n"]}],"source":["a = torch.ones((2, 3), dtype=torch.int16)\n","print(a.dtype)\n","a = torch.ones((2, 3), dtype=torch.float64)\n","print(a.dtype)\n","a = torch.ones((2, 3), dtype=torch.int8)\n","print(a.dtype)\n","a = torch.ones((2, 3), dtype=torch.int32)\n","print(a.dtype)\n","a = torch.ones((2, 3), dtype=torch.int64)\n","print(a.dtype)\n","a = torch.ones((2, 3), dtype=torch.bool)\n","print(a.dtype)\n"]},{"cell_type":"markdown","metadata":{"id":"gsJiR518Fp0A"},"source":["3. **Device** - Object representing the device on which a `torch.Tensor` is or will be allocated."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnoVRysOFp0D","outputId":"dfb0279f-6e0a-4cd9-cde1-170a57d9d940"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device tensor is initially stored on: cpu\n"]}],"source":["# Create a tensor\n","tensor = torch.rand(3, 2)\n","print(f\"Device tensor is initially stored on: {tensor.device}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BqzR-kWTFp0F"},"source":["### Operations on Tensors\n","\n","PyTorch defines over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling, and more."]},{"cell_type":"markdown","metadata":{"id":"HVh0KUGVFp0G"},"source":["**Note :**\n","Each of these operations can be run on the GPU (at typically higher speeds than on a CPU). By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using .to method (after checking for GPU availability).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEDP6zg5Fp0H","outputId":"16d6ed93-1f7f-44eb-8822-c96b5b165051"},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA is not available. Tensor remains on CPU.\n"]}],"source":["# Check if GPU is available and move tensor to GPU if it is\n","if torch.cuda.is_available():\n","    tensor = tensor.to(\"cuda\")\n","    print(f\"Device tensor is now stored on: {tensor.device}\")\n","else:\n","    print(\"CUDA is not available. Tensor remains on CPU.\")"]},{"cell_type":"markdown","metadata":{"id":"rK8U8--1Fp0J"},"source":["**Numpy-like Indexing and Slicing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NcXLu-_VFp0K","outputId":"396e9794-c2bb-47df-9605-9064c8f8d5ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["First row: tensor([1., 1., 1., 1.]) \n","\n","First column: tensor([1., 1., 1., 1.]) \n","\n","Last column: tensor([1., 1., 1., 1.]) \n","\n","tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"]}],"source":["tensor = torch.ones(4, 4)\n","print(f\"First row: {tensor[0]}\",\"\\n\")\n","print(f\"First column: {tensor[:, 0]}\",\"\\n\")\n","print(f\"Last column: {tensor[:, -1]}\",\"\\n\")\n","tensor[:,1] = 0 # set all elements in second column to 0\n","print(tensor)\n"]},{"cell_type":"markdown","metadata":{"id":"rc-JCJQ4Fp0M"},"source":["**Arithmetic operations**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcYepxamFp0N","outputId":"f8cfd97c-7769-434d-bb00-a97f14cf6e17"},"outputs":[{"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ones \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# adding 1 to a tensor of zeros\u001b[39;00m\n\u001b[1;32m      2\u001b[0m twos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# multiplying a tensor of ones by 2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m threes \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m7\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# chaining operations\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}],"source":["ones = torch.zeros(2, 2) + 1 # adding 1 to a tensor of zeros\n","twos = torch.ones(2, 2) * 2 # multiplying a tensor of ones by 2\n","threes = (torch.ones(2, 2) * 7 - 1) / 2 # chaining operations\n","fours = twos ** 2 # squaring a tensor\n","sqrt2s = twos ** 0.5 # square root of a tensor\n","\n","print(ones,\"\\n\")\n","print(twos,\"\\n\")\n","print(threes,\"\\n\")\n","print(fours,\"\\n\")\n","print(sqrt2s,\"\\n\")\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4NjJq_yYFp0Q"},"source":[" **Similar operations can be performed between two tensors:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBUaJTjRFp0S","outputId":"e5db4bbc-65b9-41c5-d6b1-7b0f2452016e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 2.,  4.],\n","        [ 8., 16.]]) \n","\n","tensor([[5., 5.],\n","        [5., 5.]]) \n","\n","tensor([[12., 12.],\n","        [12., 12.]]) \n","\n"]}],"source":["powers2 = twos ** torch.tensor([[1, 2], [3, 4]]) # element-wise exponentiation\n","print(powers2,\"\\n\")\n","fives = ones + fours # element-wise addition\n","print(fives,\"\\n\")\n","dozens = threes * fours # element-wise multiplication\n","print(dozens,\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"GtBF0WjXFp0T"},"source":["**Note :** Please make sure to check the shapes of the tensors before performing these operations, otherwise they'll result in errors."]},{"cell_type":"markdown","metadata":{"id":"ahGYZMlfFp0U"},"source":["**Tensor Broadcasting**\n","\n","The exception to the same-shapes rule is tensor broadcasting. Broadcasting is a feature in PyTorch (and other numerical libraries like NumPy) that allows operations between tensors of different shapes. It automatically adjusts the shapes of tensors to make their dimensions compatible for element-wise operations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vn1XbuUdFp0V","outputId":"0e4d6672-4d1a-4ee0-a587-650759bef8b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.2569, 0.8153, 0.3774, 0.6209],\n","        [0.8973, 0.6783, 0.8594, 0.6721]])\n","tensor([[0.5138, 1.6306, 0.7549, 1.2419],\n","        [1.7946, 1.3567, 1.7187, 1.3442]])\n"]}],"source":["rand = torch.rand(2, 4) # random tensor\n","doubled = rand * (torch.ones(1, 4) * 2) # broadcasting\n","print(rand)\n","print(doubled)\n"]},{"cell_type":"markdown","metadata":{"id":"yWQgYKngFp0X"},"source":["In the above example , the 1x4 tensor is multiplied by both rows of the 2x4 tensor.\n","\n","The rules for broadcasting are:\n","1. Each tensor must have at least one dimension - no empty tensors.\n","2. Comparing the dimension sizes of the two tensors, _going from last to first:\n","\n","    - Each dimension must be equal, or\n","    - One of the dimensions must be of size 1, or\n","    - The dimension does not exist in one of the tensors.\n","\n","Tensors of identical shape, of course, are trivially “broadcastable”.\n","\n","Here are some examples of situations that honor the above rules and allow broadcasting:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fqeC4hm8Fp0Y","outputId":"997ad001-c325-463c-fd5f-a12590635587"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[0.4544, 0.6353],\n","         [0.5914, 0.9689],\n","         [0.5715, 0.7136]],\n","\n","        [[0.4544, 0.6353],\n","         [0.5914, 0.9689],\n","         [0.5715, 0.7136]],\n","\n","        [[0.4544, 0.6353],\n","         [0.5914, 0.9689],\n","         [0.5715, 0.7136]],\n","\n","        [[0.4544, 0.6353],\n","         [0.5914, 0.9689],\n","         [0.5715, 0.7136]]]) \n","\n","tensor([[[0.4606, 0.7056],\n","         [0.4606, 0.7056],\n","         [0.4606, 0.7056]],\n","\n","        [[0.5921, 0.0557],\n","         [0.5921, 0.0557],\n","         [0.5921, 0.0557]],\n","\n","        [[0.3866, 0.2989],\n","         [0.3866, 0.2989],\n","         [0.3866, 0.2989]],\n","\n","        [[0.8993, 0.2320],\n","         [0.8993, 0.2320],\n","         [0.8993, 0.2320]]]) \n","\n","tensor([[[0.2450, 0.1956],\n","         [0.8150, 0.1024],\n","         [0.2813, 0.7884]],\n","\n","        [[0.2450, 0.1956],\n","         [0.8150, 0.1024],\n","         [0.2813, 0.7884]],\n","\n","        [[0.2450, 0.1956],\n","         [0.8150, 0.1024],\n","         [0.2813, 0.7884]],\n","\n","        [[0.2450, 0.1956],\n","         [0.8150, 0.1024],\n","         [0.2813, 0.7884]]]) \n","\n"]}],"source":["a = torch.ones(4, 3, 2)\n","b = a * torch.rand(3, 2)\n","print(b, \"\\n\") # 3rd & 2nd dims identical to a, dim 1 absent\n","c = a * torch.rand(4, 1, 2)\n","print(c, \"\\n\") # 3rd dim = 1, 2nd dim identical to a\n","d = a * torch.rand(1, 3, 2)\n","print(d, \"\\n\") # 3rd dim identical to a, 2nd dim = 1\n"]},{"cell_type":"markdown","metadata":{"id":"rj3xVXI-Fp0a"},"source":["##### **Common Functions**"]},{"cell_type":"markdown","metadata":{"id":"AJRSxpiQFp0b"},"source":["**Mathematical Functions**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-eUpGK2Fp0d","outputId":"3940133a-c495-49ca-9b0c-d16964c20e3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.8933, 0.1511, 0.7021, 0.4549],\n","        [0.2857, 0.2995, 0.6397, 0.6817]]) \n","\n","tensor([[-0., -0., -0., -0.],\n","        [1., -0., 1., -0.]]) \n","\n","tensor([[-1., -1., -1., -1.],\n","        [ 0., -1.,  0., -1.]]) \n","\n","tensor([[-0.5000, -0.1511, -0.5000, -0.4549],\n","        [ 0.2857, -0.2995,  0.5000, -0.5000]])\n"]}],"source":["import math\n","\n","a = torch.rand(2, 4) * 2 - 1 # random tensor in range [-1, 1]\n","print(torch.abs(a),\"\\n\") #absolute value\n","print(torch.ceil(a),\"\\n\") #ceiling\n","print(torch.floor(a),\"\\n\") #floor\n","print(torch.clamp(a, -0.5, 0.5)) #clamping: values < -0.5 set to -0.5, values > 0.5 set to 0.5\n"]},{"cell_type":"markdown","metadata":{"id":"fSxY9Wl2Fp0e"},"source":["**Trigonometric Functions and Their Inverses**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wxZcRcL1Fp0f","outputId":"994ed91d-4dcd-4f14-b11d-43363e81a989"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Sine and arcsine:\n","tensor([0.0000, 0.7854, 1.5708, 2.3562]) \n","\n","tensor([0.0000, 0.7071, 1.0000, 0.7071]) \n","\n","tensor([0.0000, 0.7854, 1.5708, 0.7854]) \n","\n"]}],"source":["angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4]) # angles in radians\n","sines = torch.sin(angles)\n","inverses = torch.asin(sines)\n","print('\\nSine and arcsine:')\n","print(angles,\"\\n\")\n","print(sines,\"\\n\")\n","print(inverses,\"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"2Z5aJm-oFp0g"},"source":["**Bitwise Operations**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_b3KxR8Fp0h","outputId":"f2dc376e-7409-473e-872f-7da2bf670dee"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([3, 2, 1])\n"]}],"source":["b = torch.tensor([1, 5, 11])\n","c = torch.tensor([2, 7, 10])\n","print(torch.bitwise_xor(b, c))\n"]},{"cell_type":"markdown","metadata":{"id":"h5UlSVNRFp0j"},"source":["**Comparisons**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQKpFUWjFp0j","outputId":"1afd3f92-4847-427b-cbe3-9674c55191ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Broadcasted, element-wise equality comparison:\n","tensor([[1., 2.],\n","        [3., 4.]]) \n","\n","tensor([[1., 1.]]) \n","\n","tensor([[ True, False],\n","        [False, False]])\n"]}],"source":["print('\\nBroadcasted, element-wise equality comparison:')\n","d = torch.tensor([[1., 2.], [3., 4.]])\n","print(d,\"\\n\")\n","e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n","print(e,\"\\n\")\n","print(torch.eq(d, e))  # returns a tensor of type bool\n"]},{"cell_type":"markdown","metadata":{"id":"p94xvUB3Fp0l"},"source":["**Reductions**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vl3_Z-dEFp0m","outputId":"dfa32f95-dcb6-476d-f54e-d66d3d653768"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(4.)\n","4.0\n","tensor(2.5000)\n","tensor(1.2910)\n","tensor(24.)\n","tensor([1, 2])\n"]}],"source":["d = torch.tensor([[1., 2.], [3., 4.]])\n","print(torch.max(d))\n","print(torch.max(d).item())\n","print(torch.mean(d))\n","print(torch.std(d))\n","print(torch.prod(d))\n","print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2])))  # filter unique elements\n"]},{"cell_type":"markdown","metadata":{"id":"qG_nXBv9Fp0o"},"source":["**Vector and Linear Algebra Operations**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Go3Rwg8NFp0p","outputId":"e6a5eaef-830c-47e1-e29a-795df6640f94"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0., 0., 1.]) \n","\n","tensor([[0.0902, 0.7986],\n","        [0.4120, 0.8499]]) \n","\n","tensor([[0.2705, 2.3959],\n","        [1.2360, 2.5498]]) \n","\n","torch.return_types.svd(\n","U=tensor([[-0.6444, -0.7647],\n","        [-0.7647,  0.6444]]),\n","S=tensor([3.6687, 0.6192]),\n","V=tensor([[-0.3051,  0.9523],\n","        [-0.9523, -0.3051]])) \n","\n"]}],"source":["v1 = torch.tensor([1., 0., 0.])  # x unit vector\n","v2 = torch.tensor([0., 1., 0.])  # y unit vector\n","\n","print(torch.cross(v1, v2),\"\\n\")  # returns cross product of vectors v1 and v2\n","\n","m1 = torch.rand(2, 2)  # random matrix\n","m2 = torch.tensor([[3., 0.], [0., 3.]])  # three times identity matrix\n","print(m1,\"\\n\")\n","m3 = torch.matmul(m1, m2)  # same as m3 = m1@m2\n","print(m3,\"\\n\")\n","print(torch.svd(m3),\"\\n\")  # singular value decomposition\n"]},{"cell_type":"markdown","metadata":{"id":"0vVsWfMPFp0r"},"source":["#### **Altering Tensors in Place**\n","\n","Most binary operations on tensors will return a third, new tensor.When we say `c = a * b`\n","(where a and b are tensors), the new tensor c will occupy a region of memory distinct from the\n","other tensors.\n","\n","There are times, though, that you may wish to alter a tensor in place. For this, most of the math\n","functions have a version with an appended underscore `( _ )` that will alter a tensor in place."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H-4Lyr6FFp0s","outputId":"1b5625b7-584f-4fdf-cdf5-d5c3a4b87dbe"},"outputs":[{"name":"stdout","output_type":"stream","text":["a:\n","tensor([0.0000, 0.7854, 1.5708, 2.3562])\n","tensor([0.0000, 0.7071, 1.0000, 0.7071])\n","tensor([0.0000, 0.7854, 1.5708, 2.3562])\n","\n","b:\n","tensor([0.0000, 0.7854, 1.5708, 2.3562])\n","tensor([0.0000, 0.7071, 1.0000, 0.7071])\n","tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"]}],"source":["a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n","print('a:')\n","print(a)\n","print(torch.sin(a))  # this operation creates a new tensor in memory\n","print(a)  # a has not changed\n","\n","b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n","print('\\nb:')\n","print(b)\n","print(torch.sin_(b))  # note the underscore\n","print(b)  # b has changed\n"]},{"cell_type":"markdown","metadata":{"id":"JOrbEqd2Fp0u"},"source":["**For Arithmetic Operations:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTK44iSqFp0v","outputId":"e0de3cff-3109-467c-ae81-43f7a73a4dd6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Before:\n","tensor([[1., 1.],\n","        [1., 1.]])\n","tensor([[0.5300, 0.3159],\n","        [0.7081, 0.1727]])\n","\n","After adding:\n","tensor([[1.5300, 1.3159],\n","        [1.7081, 1.1727]])\n","tensor([[1.5300, 1.3159],\n","        [1.7081, 1.1727]])\n","tensor([[0.5300, 0.3159],\n","        [0.7081, 0.1727]])\n","\n","After multiplying\n","tensor([[0.2809, 0.0998],\n","        [0.5014, 0.0298]])\n","tensor([[0.2809, 0.0998],\n","        [0.5014, 0.0298]])\n"]}],"source":["a = torch.ones(2, 2)\n","b = torch.rand(2, 2)\n","print('Before:')\n","print(a)\n","print(b)\n","print('\\nAfter adding:')\n","print(a.add_(b))\n","print(a)\n","print(b)\n","print('\\nAfter multiplying')\n","print(b.mul_(b))\n","print(b)\n"]},{"cell_type":"markdown","metadata":{"id":"IwJrwegoFp0x"},"source":["Using the `out` Argument for In-Place Computations:\n","\n","There is another option for placing the result of a computation in an existing, allocated tensor. Many of the methods and functions we’ve seen so far - including creation methods! - have an `out` argument that lets you specify a tensor to receive the output. If the out tensor is the correct shape and `dtype` , this can happen without a new memory allocation:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BB18si9Fp0z","outputId":"960d3a69-adf0-409d-c41e-b633006a32b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0., 0.],\n","        [0., 0.]]) \n","\n","tensor([[0.4621, 0.5195],\n","        [0.1035, 0.1086]]) \n","\n","tensor([[0.7443, 0.7234],\n","        [0.7417, 0.6321]]) \n","\n"]}],"source":["a = torch.rand(2, 2)\n","b = torch.rand(2, 2)\n","c = torch.zeros(2, 2)\n","old_id = id(c)\n","print(c,\"\\n\")\n","d = torch.matmul(a, b, out=c)\n","print(c,\"\\n\") # contents of c have changed\n","\n","assert c is d # test c & d are same object, not just containing equal values # make sure that our new c is the same object as the old one\n","assert id(c) == old_id\n","\n","torch.rand(2, 2, out=c) # works for creation too!\n","print(c,\"\\n\") # c has changed again\n","\n","assert id(c) == old_id # still the same object!\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xpdJjh8vFp01"},"source":["#### **Manipulating Tensor Shape**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s10aP8U_Fp02","outputId":"9f6b581c-e02b-44a2-e130-129eb1caf194"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2],\n","        [3, 4]]) \n","\n","tensor([[[0.2893, 0.1629, 0.1285,  ..., 0.2148, 0.4567, 0.4887],\n","         [0.4287, 0.6620, 0.0410,  ..., 0.0035, 0.2118, 0.5679],\n","         [0.5858, 0.9733, 0.3692,  ..., 0.6938, 0.0219, 0.6840],\n","         ...,\n","         [0.7916, 0.7293, 0.6940,  ..., 0.1591, 0.5954, 0.1723],\n","         [0.3036, 0.3075, 0.0482,  ..., 0.1157, 0.1832, 0.4325],\n","         [0.0244, 0.9426, 0.9898,  ..., 0.9208, 0.1439, 0.2637]]]) \n","\n","torch.Size([1, 20]) \n","\n","torch.Size([20]) \n","\n","tensor([[0.2854, 0.0043, 0.2710],\n","        [0.6577, 0.9595, 0.6804],\n","        [0.7525, 0.3462, 0.6816],\n","        [0.7475, 0.8048, 0.6805],\n","        [0.6754, 0.7080, 0.1422],\n","        [0.7200, 0.5164, 0.3350]])\n"]}],"source":["a = torch.tensor([1, 2, 3, 4])\n","a_reshaped = torch.reshape(a, (2, 2))\n","print(a_reshaped,\"\\n\")\n","\n","b = torch.rand(56, 56)  # Consider 56x56 image\n","c = b.unsqueeze(0)  # unsqueeze(i) adds dimension of length 1 at index i\n","print(c,\"\\n\")  # c is now a batch of 1 image of shape 56x56\n","\n","d = torch.rand(1, 20)\n","print(d.shape,\"\\n\")\n","e = d.squeeze(0)  # squeeze(i) removes a dimension if shape[i] is 1\n","print(e.shape,\"\\n\")\n","\n","x, y, z = torch.rand(2, 3), torch.rand(2, 3), torch.rand(2, 3)\n","cat_tensor = torch.cat((x, y, z), dim=0)  # concatenates tensors along rows\n","print(cat_tensor)\n"]},{"cell_type":"markdown","metadata":{"id":"HKmdTwCEFp05"},"source":["**Note:** The way to understand the “axis” or \"dim\" of a torch function is that it collapses the specified axis."]},{"cell_type":"markdown","metadata":{"id":"-di-_NHmFp06"},"source":["**Copying Tensors**\n","\n","Assigning a tensor to a variable makes the variable a ***label*** of the tensor, and does not ***copy*** it.\n","\n","For example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Dkz3gccFp07","outputId":"f42e6900-14f5-4b73-86cb-01358fe070fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[  1., 561.],\n","        [  1.,   1.]])\n"]}],"source":["a = torch.ones(2, 2)\n","b=a\n","a[0][1] = 561 # we change a...\n","print(b) # ...and b is also altered\n"]},{"cell_type":"markdown","metadata":{"id":"HrhXPahdFp08"},"source":["But what if you want a separate copy of the data to work on? The `clone()` method is there for you:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ld15z0KgFp09","outputId":"f9f5b722-21e5-4021-e7ad-9faee3ed4fed"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[True, True],\n","        [True, True]])\n","tensor([[1., 1.],\n","        [1., 1.]])\n"]}],"source":["a = torch.ones(2, 2)\n","b = a.clone()\n","\n","assert b is not a # different objects in memory...\n","print(torch.eq(a,b)) # ...but still with the same contents!\n","a[0][1] = 561 # a changes...\n","print(b)   # ...but b is still all ones\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3jT4bkkNFp0_"},"source":["#### **Moving to GPU**"]},{"cell_type":"markdown","metadata":{"id":"H_8QJ6ZyFp1A"},"source":["First, check whether a GPU is available:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liazPooyFp1B","outputId":"b230cc03-15ee-4b9f-9914-d0febe65d6f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["MPS available!\n"]}],"source":["if torch.cuda.is_available():\n","    print('We have a GPU!')\n","else:\n","    try:\n","        device = torch.device(\"mps\")\n","        print(\"MPS available!\")\n","    except:\n","        print('Sorry, CPU only.')\n"]},{"cell_type":"markdown","metadata":{"id":"6LOGGF7EFp1D"},"source":["You can move your tensor to the GPU using the `.to()` method:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PyUDJGhNFp1E","outputId":"d74d3edb-56f9-4c30-d5cd-c220c3cf882c"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.1850, 0.7680],\n","        [0.1002, 0.3113]], device='mps:0')\n"]}],"source":["if torch.cuda.is_available():\n","    my_device = torch.device('cuda')\n","else:\n","    try:\n","        my_device = torch.device(\"mps\")\n","    except:\n","        my_device = torch.device('cpu')\n","\n","x = torch.rand(2, 2, device=my_device)\n","print(x)\n"]},{"cell_type":"markdown","metadata":{"id":"rszq0T_lFp1G"},"source":["If you have an existing tensor living on one device, you can move it to another with the `to()` method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QPqUjaNOFp1H"},"outputs":[],"source":["y = torch.rand(2, 2)\n","y = y.to(my_device)\n"]},{"cell_type":"markdown","metadata":{"id":"5fgkf4ELFp1I"},"source":["### **Numpy Bridge**\n","\n","If you have existing ML or scientific code with data stored in NumPy `ndarrays`, you may wish to express that same data as PyTorch tensors, whether to take advantage of PyTorch’s GPU acceleration, or its efficient abstractions for building ML models. It’s easy to switch between `ndarrays` and PyTorch `tensors`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kh-Wlmm9Fp1J","outputId":"dc414e56-269e-49c8-e823-9d54a259f9f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["NumPy Array:\n","[[1. 1. 1.]\n"," [1. 1. 1.]]\n","\n","PyTorch Tensor:\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.]], dtype=torch.float64)\n","\n","Random PyTorch Tensor:\n","tensor([[0.0948, 0.9340, 0.8146],\n","        [0.6878, 0.7209, 0.7132]])\n","\n","Converted NumPy Array:\n","[[0.09482563 0.9340007  0.81461215]\n"," [0.68782526 0.7209366  0.71318144]]\n","\n","Updated PyTorch Tensor from NumPy Array:\n","tensor([[ 1.,  1.,  1.],\n","        [ 1., 23.,  1.]], dtype=torch.float64)\n","\n","Updated NumPy Array from PyTorch Tensor:\n","[[ 0.09482563  0.9340007   0.81461215]\n"," [ 0.68782526 17.          0.71318144]]\n"]}],"source":["import numpy as np\n","import torch\n","\n","# Creating a NumPy array\n","numpy_array = np.ones((2, 3))\n","print(\"NumPy Array:\")\n","print(numpy_array)\n","\n","# Converting NumPy array to PyTorch tensor\n","pytorch_tensor = torch.from_numpy(numpy_array)\n","print(\"\\nPyTorch Tensor:\")\n","print(pytorch_tensor)\n","\n","# Creating a random PyTorch tensor\n","pytorch_rand = torch.rand(2, 3)\n","print(\"\\nRandom PyTorch Tensor:\")\n","print(pytorch_rand)\n","\n","# Converting PyTorch tensor to NumPy array\n","numpy_rand = pytorch_rand.numpy()\n","print(\"\\nConverted NumPy Array:\")\n","print(numpy_rand)\n","\n","# Changes to NumPy array reflect in the PyTorch tensor and vice versa\n","numpy_array[1, 1] = 23\n","print(\"\\nUpdated PyTorch Tensor from NumPy Array:\")\n","print(pytorch_tensor)\n","\n","pytorch_rand[1, 1] = 17\n","print(\"\\nUpdated NumPy Array from PyTorch Tensor:\")\n","print(numpy_rand)\n"]},{"cell_type":"markdown","metadata":{"id":"Lq0h-8ozFp1L"},"source":["It is important to know that these converted objects are using the ***same underlying memory*** as their\n","source objects, meaning that changes to one are reflected in the other:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VyqjvLkzFp1M","outputId":"970d0d5c-5d11-4e5f-8124-85c567fbe6f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 1.,  1.,  1.],\n","        [ 1., 23.,  1.]], dtype=torch.float64) \n","\n","[[ 0.09482563  0.9340007   0.81461215]\n"," [ 0.68782526 17.          0.71318144]]\n"]}],"source":["numpy_array[1, 1] = 23\n","print(pytorch_tensor,\"\\n\")\n","pytorch_rand[1, 1] = 17\n","print(numpy_rand)"]},{"cell_type":"markdown","metadata":{"id":"wFB8Lq9HFp1N"},"source":["### **Questions**\n","\n","Given below are a few questions for you to solve. Do solve them as it will help reinforce the concepts we've learnt so far\n"]},{"cell_type":"markdown","metadata":{"id":"0M5vnsvPFp1P"},"source":["1. Obtain a tensor containing only zeroes from the given tensor"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"aP2_7qk4Fp1R"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0, 0, 0, 0],\n","        [0, 0, 0, 0],\n","        [0, 0, 0, 0],\n","        [0, 0, 0, 0]])\n"]}],"source":["pattern = torch.tensor([\n","    [1, 1, 1, 1],\n","    [1, 0, 0, 1],\n","    [1, 0, 0, 1],\n","    [1, 1, 1, 1]\n","])\n","zeros=torch.zeros_like(pattern)\n","print(zeros)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ywx-0aCmOC3W"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"HqRbw2LXFp1S"},"source":["2. Create a Numpy array of shape (3,2,3) using PyTorch\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"_1BuSoBNFp1T"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1. 1. 1.]\n"," [1. 1. 1.]\n"," [1. 1. 1.]]\n"]}],"source":["np_array=torch.ones(3,3).numpy()\n","print(np_array)"]},{"cell_type":"markdown","metadata":{"id":"lz1R-1sIFp1U"},"source":["3. Create two random (2,3,3) tensors and find the max, min, mean, std of their product (matrix multiplication)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"sSxJ0oOrFp1V"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(0.9877)\n","tensor(0.4144)\n","tensor(0.7029)\n","tensor(0.2112)\n"]}],"source":["a=torch.rand(2,2,2)\n","b=torch.rand(2,2,2)\n","\n","print(torch.max(torch.matmul(a,b)))\n","print(torch.min(torch.matmul(a,b)))\n","print(torch.mean(torch.matmul(a,b)))\n","print(torch.std(torch.matmul(a,b)))"]},{"cell_type":"markdown","metadata":{"id":"VIeb1VibFp1X"},"source":["4. Convert a 16x16 tensor into 1x256 tensor\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"7OifkwqGFp1Y"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1.]])\n"]}],"source":["a=torch.ones(16,16)\n","b=torch.reshape(a,(1,256))\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"ldkMkMERFp1Z"},"source":["5. Given two tensors x and Y, find the coefficients that best model the linear relationship `Y = ax + b` (Linear Regression)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bhR9YX0Fp1a"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"H-WD2t18cdLR"},"source":["6.Perform element-wise multiplication and addition on two 3x3 tensors."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"so_6olTDc4Gi"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.3454, 0.0155, 0.2062],\n","        [0.3938, 0.4538, 0.5226],\n","        [0.0731, 0.6994, 0.3427]])\n","tensor([[1.3022, 0.4664, 0.9185],\n","        [1.3037, 1.4499, 1.5057],\n","        [0.5953, 1.6759, 1.3362]])\n"]}],"source":["a=torch.rand(3,3)\n","b=torch.rand(3,3)\n","\n","print(a*b)\n","print(a+b)"]},{"cell_type":"markdown","metadata":{"id":"UaRIZpVPc_jA"},"source":["7.Stack two 2x3 tensors along a new dimension."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"3xTW0RxvdIAb"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[0.2502, 0.9350, 0.2533],\n","         [0.1537, 0.0902, 0.8127]],\n","\n","        [[0.6942, 0.5577, 0.4585],\n","         [0.8750, 0.3673, 0.2642]]])\n"]}],"source":["a=torch.rand(2,3)\n","b=torch.rand(2,3)\n","\n","print(torch.stack((a,b),dim=0))"]},{"cell_type":"markdown","metadata":{"id":"IFOAb3YcdUCd"},"source":["8.Create a 1D tensor with values ranging from 0 to 9."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"66M2z00gdXhz"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"]}],"source":["a=torch.arange(10)\n","print(a)"]},{"cell_type":"markdown","metadata":{"id":"NohW8vX7dX77"},"source":["9.Perform operations on tensors of different shapes: 2x3 and 1x3 tensor using broadcasting."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"NWqjrfoNekNS"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.8508, 0.2486, 0.2543],\n","        [0.6092, 0.2445, 0.0899]])\n","tensor([[1.8472, 1.0952, 1.0628],\n","        [1.5985, 1.0899, 0.6109]])\n"]}],"source":["a=torch.rand(2,3)\n","b=torch.rand(1,3)\n","\n","print(a*b)\n","print(a+b)"]},{"cell_type":"markdown","metadata":{"id":"UL6SMnYAjpaf"},"source":["10.Reshape a 1D tensor with 12 elements into a 3x4 matrix.  give answer without comments\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"3PmT_pN8juW9"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.2385, 0.9773, 0.3437, 0.5615],\n","        [0.4845, 0.7308, 0.0241, 0.4325],\n","        [0.9971, 0.3832, 0.5409, 0.6595]])\n"]}],"source":["a=torch.rand(12)\n","b=torch.reshape(a,(3,4))\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"ujgOX8Y4Fp1a"},"source":["In this notebook, we covered the fundamental aspects of PyTorch and tensors, starting from installation to performing various operations.\n","By mastering these basics, you now have a solid foundation to build and train more complex machine learning models using PyTorch. Continue exploring PyTorch’s extensive library of functions and tools to further enhance your skills and tackle more advanced topics in deep learning and artificial intelligence.\n","\n","Feel free to refer to the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for more detailed information and examples. Happy coding!"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"pytorch","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
